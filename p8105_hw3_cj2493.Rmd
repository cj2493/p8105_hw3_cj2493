---
title: "p8105_hw3_cj2493"
author: "Courtney Johnson"
date: "October 4, 2018"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
library(tidyverse)
library(ggplot2)
library(openintro)
library(p8105.datasets)
library(patchwork)
```

## Problem 1

Load BRFSS data from the p8105.datasets package and clean:

```{r, load_brfss}
data(brfss_smart2010) 
brfss_smart2010 = janitor::clean_names(brfss_smart2010) %>%
  filter(topic == "Overall Health") %>%
  select(-class, -topic, -question, -sample_size, -c(confidence_limit_low:geo_location)) %>%
  mutate(response = factor(response, labels = c("Excellent", "Very Good", "Good", "Fair", "Poor")))
```

Some problem solving using the data:

```{r, manipulation}
brfss_2002 = filter(brfss_smart2010, year == 2002) %>%
  distinct(locationdesc, .keep_all = TRUE) %>%
  count(locationabbr) %>%
  filter(n == 7) %>%
  mutate(state = abbr2state(locationabbr)) %>%
  select(-locationabbr)
```

In 2002, the states that had 7 locations represented were the following: `r brfss_2002$state`.

Make a "spaghetti plot" that shows the number of locations in each state from 2002 to 2010:

```{r, spaghetti_plot}
brfss_spaghetti = brfss_smart2010 %>%
  group_by(year, locationabbr) %>%
  summarize(n_locations = n_distinct(locationdesc)) %>%
  ggplot(aes(x = year, y = n_locations, color = locationabbr)) + 
  labs(x = "Year", y = "Number of Locations", color = "State") +
  geom_line(size = 1) +
  theme(legend.position = "right")
brfss_spaghetti
```

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of "Excellent"
 responses across locations in NY state.
 
```{r, table}
brfss_table_data = filter(brfss_smart2010, year == 2002 | year == 2006 | year == 2010) %>%
  filter(locationabbr == "NY") %>%
  filter(response == "Excellent") %>%
  group_by(year) %>%
  summarize(mean_excellent = mean(data_value),
         sd_excellent = sd(data_value)) %>%
  knitr::kable()

brfss_table_data
```

For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five panel plot that shows, for each response category separately, the distribution of these state level averages over time. 

```{r, five_panel_plot}
brfss_plot = brfss_smart2010 %>%
  group_by(year, locationabbr, response) %>%
  summarize(mean_prop = mean(data_value)) %>%
  ggplot(aes(x = year, y = mean_prop)) + 
  geom_point() + 
  facet_grid(~response) +
  labs(x = "Year", y = "Mean Proportion", title = "Change in Mean Proportions Over Time by Response") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

brfss_plot
```


## Problem 2

Load the instacart data from the p8105.datasets package:

```{r, load_instacart}
data(instacart) 
instacart = janitor::clean_names(instacart)
```

Write a description of the dataset:

The instacart data has `r nrow(instacart)` observations of `r ncol(instacart)` variables. Some of the variables include product name, the product id, the order in which it was added to the cart, what hour it was added, the id of its department, and number of days since the last time it was ordered. For example, here is a chunk of the dataset: 

```{r, example}
head(instacart)
```

In the first row, the item is Bulgarian yogurt, was part of order 1, had id 49302, was the first item added to the cart, was ordered 9 days prior, and was located in aisle 120. There are `r nrow(count(instacart, aisle_id))` aisles, and the most items are ordered from aisle `r which.max(count(instacart, aisle_id)$n)`.

Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it. 

```{r, aisle_plot}
instacart_num_items = instacart %>%
  group_by(aisle) %>%
  summarize(items_ordered = n()) %>%
  ggplot(aes(x = aisle, y = items_ordered)) +
  geom_bar(stat = "identity") +
  labs(x = "Aisle", y = "Number of Items Ordered", title = "Number of Items Ordered in Each Aisle") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))


instacart_num_items
```

Make a table showing the most popular item in each of the aisles "baking ingredients", "dog food care", and "packaged vegatables fruits".

```{r, pop_aisles}
pop_aisles = filter(instacart, aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%
  group_by(aisle, product_name) %>%
  summarize(product_count = n()) %>%
  filter(product_count == max(product_count)) %>%
  knitr::kable()
pop_aisles
```



Make a table showing the mean hour of the day at which pink lady apples and coffee ice cream are ordered on each day of the week. 

```{r, mean_hour_table}
mean_hour = instacart %>%
  filter(product_name == "Pink Lady Apple" | product_name == "Coffee Ice Cream") %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable()

mean_hour
```


## Problem 3

Load the ny_noaa data from the p8105.datasets package:

```{r, load_ny_noaa}
data(ny_noaa)
ny_noaa = janitor::clean_names(ny_noaa)
```

Describe the data:

Clean the data:

```{r, clean_ny_noaa}
tidy_ny_noaa = separate(ny_noaa, date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(year = as.numeric(year), 
         month = as.numeric(month), 
         day = as.numeric(day), 
         tmax = as.numeric(tmax)/10, 
         tmin = as.numeric(tmin)/10,
         prcp = prcp/10
         )
```

Make a two-panel plot showing the average max temperature in January and in July in each station across years.

```{r}
jan_jul_ny_noaa = filter(tidy_ny_noaa, month == 1 | month == 7) %>%
  mutate(month = month.name[month]) %>%
  group_by(year, month) %>%
  summarize(mean_tmax = mean(tmax, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = mean_tmax)) +
  geom_line() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(x = "Year", y = "Mean Maximum Temperature, in Celsius", title = "Change in Temperature in New York") + 
  facet_grid(~month) 

jan_jul_ny_noaa
```

Make a two panel plot (i) showing tmax vs tmin for the full dataset, and (ii) showing the distribution of snowfall values greater than 0 and less than 100 separately by year

```{r, two_panel}
tmax_vs_tmin = tidy_ny_noaa %>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_smooth()

snow_dist = filter(tidy_ny_noaa, snow > 0 & snow < 100) %>%
  mutate(year = as.character(year)) %>%
  group_by(year) %>%
  ggplot(aes(x = snow, fill = year)) +
  geom_density(alpha = .3)  

tmax_vs_tmin + snow_dist
```



